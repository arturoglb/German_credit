
## **Analysis**

```{r, echo = FALSE, message = FALSE, warning=FALSE}
source(here::here("script/setup.R"))
```

### **Data Preparation for Models**

Before starting applying the models to the data, first we started converting the categorical variables from numeric(int) values to categorical (factor). For example, CHK_ACCT is actually a categorical data but the values in german_credit are numeric (int). On the following pie chart we can observe this transformation. 

```{r message=FALSE, include=FALSE}
#convert categorical data to factor
german_credit[,c(catevar)] <- lapply(german_credit[,c(catevar)], factor)

str(german_credit)
```

```{r echo=FALSE, message=FALSE}
x<-inspect_types(german_credit[,-p])
show_plot(x, col_palette=2)
```

Only for our `RESPONSE` variable, we transform the values to "Good" if it is equal to 1, and "Bad" otherwise. We allocated this new values in a column called `Applicant`. It is worth mentioning that we could have treated the data the way it was "0 & 1", and applied a regression task, but we preferred to visualize the data with good and bad values.

Second, we set a seed value for reproducibility purposes for the data partitioning.

Finally, we proceed splitting the German Credit data into two datasets, to ensure that the models will not overfit the data and that the results of the predictions are good. To do so, we select for the first set; our **training set**, 80% of the observations randomly(*800 obs*), and for the observations that remain we took them as our **test set**(*200 obs*).



```{r message=FALSE, warning=FALSE, include=FALSE}
German_data <- german_credit %>% mutate(Applicant =ifelse(RESPONSE == 1, "Good", "Bad"))
German_data <- German_data %>% select(-RESPONSE)
German_data$Applicant <- as.factor(German_data$Applicant)

set.seed(123) # for reproducibility
index.tr <- createDataPartition(y = German_data$Applicant, p= 0.8, list = FALSE)
df.tr <- German_data[index.tr,]
df.te <- German_data[-index.tr,]
```

### **Models**

Our goal is to obtain a model that may be used to determine if new applicants present a good or bad credit risk. Since we have transformed the column `RESPONSE` as a factor with categorical values, we will apply models that consider a classification task. 

We have chosen the models as follows:

1. Decision Trees
2. Random Forest
3. Logistic Regression
4. Neural Networks

### **Decision Trees - Classification**

Decision trees are algorithms that recursively search the space for the best boundary possible, until we unable them to do so (Ivo Bernardo,2021). The basic functionality of decision trees is to split the data space into rectangles, by measuring each split. The main goal is to minimize the impurity of each split from the previous one. 

> **Build the model** - Unbalanced data without Cross validation

```{r message=FALSE, warning=FALSE, include=FALSE}
# Classification tree fit and plot
german.tree <- rpart(Applicant ~ ., method= "class", 
                     data=df.tr, cp= 0.001, model=TRUE)
summary(german.tree)
```

After creating the model with `rpart` function, we proceed to plot it to visualize the result of this classification tree. In the graph we can observe that the main splitting variable is "CHK_ACCT = 0,1" the selected one, it reduces the impurity by **43.99**. In addition, we can determine which variables have the most significant reduction impact on the impurity function; in this case they will be those with the longest splitting length.

* The variables are:
    + HISTORY: Credit **History**
    + DURATION: **Duration** of the Credit

```{r echo=FALSE, message=FALSE}
par(mar = c(0.5, 1, 0.5, 1))
plot(german.tree, branch = 1)
text(german.tree, digits = 1, use.n = TRUE, cex = 0.6, pretty=1)

#rpart.plot(german.tree, cex = 0.5)
```

> **Pruning the Tree**

We decided to prune the tree to reduce the statistical noise in the data, because as the tree splits over and over the length becomes shorter, therefore the importance of the split diminishes. Another reason is that since decision trees are susceptible to overfitting, reducing the size of the model will improve the accuracy.

In the Complexity table below, we can visualize the 16 variables that were considered in the construction of the classification tree. Furthermore, the tree yielding the lowest-cross-validated rate `xerror` is tree number 3. We have chosen this tree by using the rule of thumb, which chooses the **lowest** level where the `rel_error + xstd < xerror`, and also by considering the simplest tree, so we discard the trees 4, 5, and 6 for this reason.

```{r echo=FALSE, message=FALSE}
printcp(german.tree)
```

Another way to find the lowest-cross-validated rate is by visualizing the size of the tree on a plot, in which you can observe the relative error `rel error` on the y axis, the cross-validation procedure `cp` on the x axis, and on the top side of the plot,the size of the tree (no. of terminal nodes). The black line is the cross-validated error rate `xerror` of each split.

From the graph, we can visualize right away which cp to choose  we can select the one closest to the dotted line, and the simplest one. For this case we could choose the tree with 7 nodes with a cp of 0.025

```{r echo=FALSE, message=FALSE}
par(pty="s")
plotcp(german.tree)
```

To visualize how the classification tree will look-like after the pruning, we plot it again. *Note: On the following tree we considered the cp of* **0.0166667**.  

We can note from the graph that the classification tree has shortened, and the main splitting branch variable remains the same as expected, but the third node `OTHER_INSTALL` that was considered on the previous tree, has been removed, the same happened with the fourth node `SAV_ACCT` and the node `JOB`.

```{r echo=FALSE, message=FALSE}
set.seed(123)
german.prune.tree <- prune(german.tree, cp=0.0166667)
rpart.plot(german.prune.tree, cex = 0.7,
           main = "Pruned Tree without CV and Unbalanced Data",
           cex.main = 1, digits = 2, leaf.round = 9, shadow.col = "gray")
```

> **Model evaluation** - Unbalanced data without Cross validation

```{r echo=FALSE, message=FALSE}
table(df.tr$Applicant)
```

From the table we can observe an unbalanced data in the training set with 204 bad applicants and 560 good applicants, since there are many more “Good” applicants than “Bad” applicants, any model would favor the prediction of the “Good”.

First,we want to measure the accuracy of our model with the unbalance data, to know how good our model is, so we compute the confusion matrix to observe it's values. 

```{r echo=FALSE, message=FALSE}
set.seed(123)
german.pred.tree <- predict(german.prune.tree, newdata=df.te, type="class")
table(Pred= german.pred.tree, Obs=df.te$Applicant)

# Measure the accuracy of the prediction
draw_confusion_matrix1 <- function(cm) {
  
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)
  
  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 440, 'Bad', cex=1.2, font=14)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 440, 'Good', cex=1.2, font=14)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=15)
  text(245, 450, 'Actual', cex=1.3, font=15)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Bad', cex=1.2, srt=90, font=14)
  text(140, 335, 'Good', cex=1.2, srt=90, font=14)
  
  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')
  
  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS",
       xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=15)
  text(10, 65, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=15)
  text(30, 65, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=15)
  text(50, 65, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=15)
  text(70, 65, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=15)
  text(90, 65, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  
  # add in the accuracy information 
  text(20, 35, names(cm$overall[1]), cex=1.5, font=15)
  text(20, 15, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(50, 35, names(cm$overall[2]), cex=1.5, font=15)
  text(50, 15, round(as.numeric(cm$overall[2]), 3), cex=1.4)
  text(80, 35, names(cm$byClass[11]), cex=1.5, font=15)
  text(80, 15, round(as.numeric(cm$byClass[11]), 3), cex=1.4)
}
c1<-confusionMatrix(german.pred.tree, df.te$Applicant)
draw_confusion_matrix1(c1)
```

We note a **74%** of accuracy, a balanced accuracy of **62%**, and disproportion of the sensitivity (37%) and specificity(90%) which is not good. For that reason, we decided to balance our data so we can improve the balanced accuracy and make the overall score more robust by applying to the model a cross-validation technique. This will help our model to find the best set of hyperparameters and have better results.


> **Class Balancing** - Re-sampling Data

Balancing by re-sampling consists of increasing the number of cases in the smallest class (here "Bad") by re-sampling at random cases from this category to get the same amount as the largest category (here "Good"). It has the same aim as sub-sampling which is to have the same amount on each category(reducing the highest category). 

After applying the re-sampling we balance the data set to 560 applicants each.

```{r echo=FALSE, message=FALSE}
n.good <- max(table(df.tr$Applicant)) ## 560

df.tr.good <- filter(df.tr, Applicant =="Good") ## the "Good" cases
df.tr.bad <- filter(df.tr, Applicant =="Bad") ## the "Bad" cases

## sub-sample 204 instances from the "Good"
index.bad <- sample(size=n.good, x=1:nrow(df.tr.bad), replace=TRUE) 

## Bind all the "Bad" and the sub-sampled "Good"
df.tr.resamp <- data.frame(rbind(df.tr.good, df.tr.bad[index.bad,]))
table(df.tr.resamp$Applicant) ## The cases are balanced
```

> **Model evaluation** - Balanced data with Cross validation

On the previous analysis we have seen how to build a decision tree model with the `rpart` function(by hand), also how to select manually the preferred CP from the complexity table, and how to identify it on a graph. Now, we will build a decision tree model with a Cross validation (CV) with the `caret`function (automatically), it will be applied to the balanced data that we created on the previous point. Finally, we will compare which balance accuracy is the highest.  

First, we split the **training data** into 10 non-overlapping subsets, 9/10 of this folds will be used to train the model, and 1/10 will be used as a validation set.
Secondly, we build the model with the data that we have already split to the function trainControl of `caret`, and plot the tree with the final Model computed.

Finally, we apply the model to the test data set to visualize the outcome on the confusion matrix table.

```{r echo=FALSE, message=FALSE}

# Building a Classification tree model: Considering a Cross-Validation with a Class balancing of Re-sampling
german.cv.resamp <- caret::train(Applicant ~ .,
                           data = df.tr.resamp,
                           method ="rpart",
                           preProcess = NULL,
                           cp = 0.001,
                           model = T,
                           trControl=trainControl(method="cv", number=10,
                                                  verboseIter=FALSE))

# Apply the trained model to the test set
german.pred.resamp.tree.cv <- predict(german.cv.resamp, newdata=df.te)

# Measure the accuracy of the prediction
draw_confusion_matrix2 <- function(cm) {
  
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)
  
  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 440, 'Bad', cex=1.2, font=14)
  rect(250, 430, 340, 370, col='brown2')
  text(295, 440, 'Good', cex=1.2, font=14)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=15)
  text(245, 450, 'Actual', cex=1.3, font=15)
  rect(150, 305, 240, 365, col='brown2')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Bad', cex=1.2, srt=90, font=14)
  text(140, 335, 'Good', cex=1.2, srt=90, font=14)
  
  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')
  
  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS",
       xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=15)
  text(10, 65, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=15)
  text(30, 65, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=15)
  text(50, 65, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=15)
  text(70, 65, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=15)
  text(90, 65, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  
  # add in the accuracy information 
  text(20, 35, names(cm$overall[1]), cex=1.5, font=15)
  text(20, 15, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(50, 35, names(cm$overall[2]), cex=1.5, font=15)
  text(50, 15, round(as.numeric(cm$overall[2]), 3), cex=1.4)
  text(80, 35, names(cm$byClass[11]), cex=1.5, font=15)
  text(80, 15, round(as.numeric(cm$byClass[11]), 3), cex=1.4)
}
c2 <-confusionMatrix(german.pred.resamp.tree.cv, df.te$Applicant)
draw_confusion_matrix2(c2)
``` 

As expected, the accuracy has decreased to **64%** but the balanced accuracy has increased to **67%**, also we can observe a better result on the specificity (75%) and the sensitivity (60) with a higher result on the Balanced Accuracy **0.675** against the previous with **0.629**. 

Overall, we determine that the model computed with the `caret`function was the one performing better on new data (*test set*) for the Classification Tree. Moreover, as we wanted to visualize the best result of the classification tree, we plot it into a graph.  

```{r echo=FALSE, message=FALSE}
rpart.plot(german.cv.resamp$finalModel, cex = 0.7,
           main = "Final Tree from CV and Balanced Data (Re-sampling)",
           cex.main = 1, digits = 2, leaf.round = 9, shadow.col = "gray")
```

We can recognize that the variables selected for the final output by order of importance were:

  * CHK ACCT
  * Duration
  * History
  * Real Estate
  
### **Random Forest** 

Random Forest (RF) are algorithms of a set of decision trees that will produce a final prediction with the average outcome of the set of trees considered (*user can define the amount of trees and the number of variables for each node*). One of the reasons that we decided to test this method is because RF are considered to be more stable than Decision Trees; more trees better performance, but certain advantages come at a price. RF slow down the computation speed and cannot be visualize, however, we will look at the results for later comparison (Saikumar Talari, 2022).

> **Model evaluation** - Balanced data with Cross validation

For this method we will consider the same approach as the last one of Classification Tree, but we will use another class balancing called Sub-sampling. Balancing by sub-sampling consists of decreasing the number of cases in the highest class (here "Good") by sub-sampling at random cases from this category to get the same amount as the smallest category (here "Bad"). It has the same aim as re-sampling which is to have the same amount on each category(*increasing the lowest category*). Finally, we will also take into account a Cross-Validation technique.

```{r echo=FALSE, message=FALSE}
# Build the model
german.rf <- caret::train(Applicant ~ .,
                         data=df.tr,
                         method="rf",
                         preProcess=NULL, 
                         trControl=trainControl(method="cv", 
                                                number=10,
                                                verboseIter=FALSE,
                                                sampling = "down"))


# Apply Model to the test dataset
german.rf.pred <- predict(german.rf, newdata=df.te)

# Measure the accuracy of the prediction
draw_confusion_matrix3 <- function(cm) {
  
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)
  
  # create the matrix 
  rect(150, 430, 240, 370, col='#339933')
  text(195, 440, 'Bad', cex=1.2, font=14)
  rect(250, 430, 340, 370, col='#CCCCCC')
  text(295, 440, 'Good', cex=1.2, font=14)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=15)
  text(245, 450, 'Actual', cex=1.3, font=15)
  rect(150, 305, 240, 365, col='#CCCCCC')
  rect(250, 305, 340, 365, col='#339933')
  text(140, 400, 'Bad', cex=1.2, srt=90, font=14)
  text(140, 335, 'Good', cex=1.2, srt=90, font=14)
  
  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')
  
  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS",
       xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=15)
  text(10, 65, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=15)
  text(30, 65, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=15)
  text(50, 65, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=15)
  text(70, 65, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=15)
  text(90, 65, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  
  # add in the accuracy information 
  text(20, 35, names(cm$overall[1]), cex=1.5, font=15)
  text(20, 15, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(50, 35, names(cm$overall[2]), cex=1.5, font=15)
  text(50, 15, round(as.numeric(cm$overall[2]), 3), cex=1.4)
  text(80, 35, names(cm$byClass[11]), cex=1.5, font=15)
  text(80, 15, round(as.numeric(cm$byClass[11]), 3), cex=1.4)
}
c3 <-confusionMatrix(german.rf.pred, df.te$Applicant)
draw_confusion_matrix3(c3)
``` 

According from the results of the Confusion matrix, we note that the Accuracy increase from **0.645** from the previous model to **0.66** in this one. There is a smaller difference between the sensitivity and the specificity, which means that the precision of the model is higher determining if an Applicant is Good or Bad. In addition, the Cohen's Kappa has a strength of agreement of **0.309** (fair agreement), which means that the observed accuracy is only slightly higher than the accuracy that one would expect from a random model. Overall, the results of Random Forest are better than the Classification Tree.



